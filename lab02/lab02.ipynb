{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "imperial-nightlife",
   "metadata": {},
   "source": [
    "# <center>CITS5508 Lab 2: Bean Classification</center>\n",
    "\n",
    "**Name:** Chitra M Saraswati<br>\n",
    "**Student ID:** 21367076<br>\n",
    "\n",
    "In this workbook we classify beans of seven different varieties. Specifically, we use a Support Vector Classifier and a Stochastic Gradient Descent Classifier; we subsequently compare the performance of these two classification methods.\n",
    "\n",
    "As per the data description:<br>\n",
    "<i>\"Seven different types of dry beans were used in this research, taking into account the features such as form, shape, type, and structure by the market situation. A computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. Bean images obtained by computer vision system were subjected to segmentation and feature extraction stages, and a total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.\"</i>\n",
    "\n",
    "You can also find the relevant paper below:<br>\n",
    "KOKLU, M. and OZKAN, I.A., (2020), “Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.” Computers and Electronics in Agriculture, 174, 105507. DOI: https://doi.org/10.1016/j.compag.2020.105507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To ensure stable output across runs\n",
    "np.random.seed(35)\n",
    "\n",
    "# For pretty plots\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_path = os.path.join('DryBeanDataset', 'Dry_Bean_Dataset.xlsx')\n",
    "os.path.exists(bean_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-radius",
   "metadata": {},
   "source": [
    "# 1. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-declaration",
   "metadata": {},
   "source": [
    "## 1.1 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "wb = load_workbook(bean_path)\n",
    "\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "sheet = wb.active\n",
    "data = sheet.values\n",
    "cols = next(data)[0:]\n",
    "df = pd.DataFrame(data, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-packaging",
   "metadata": {},
   "source": [
    "## 1.2 Exploratory Data Analysis\n",
    "\n",
    "An explanation of the attributes for this data can be found in the data description. I also thought it was a good idea to go to the paper and have a look at the beans myself. If you refer to the paper, we see that the Bombay and Seker varieties are the most visually distinctive from the other varieties due to their area and roundedness.\n",
    "\n",
    "As our ML algorithms won't be taking colour into consideration, the Barbunya beans wouldn't be as distinctive to our algorithms as we visually judge them to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-figure",
   "metadata": {},
   "source": [
    "<img src=\"bean-varieties.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-consultation",
   "metadata": {},
   "source": [
    "<img src=\"beans-all-together.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any NA values\n",
    "sample_incomplete_rows = df[df.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-reservoir",
   "metadata": {},
   "source": [
    "### Histogram of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize = (20,15), bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-tunnel",
   "metadata": {},
   "source": [
    "We discover from the histograms that there are no extreme outliers in our data for any of the attributes. However, some attributes have long tails and are left-skewed (such as that for area, perimeter, major and minor axes length, convex area and diameter). Solidity and ShapeFactor4 are right-skewed.\n",
    "\n",
    "Additionally, there are some instances of bimodal distributions (such as that for ShapeFactor1) and multimodal distributions (such as that for compactness and ShapeFactor3).\n",
    "\n",
    "These measures indicate that the data aren't necessarily normally distributed for these features, so we should not assume a normal distribution when selecting the appropriate models/algorithms for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-warren",
   "metadata": {},
   "source": [
    "### Scatter plot matrix of most interesting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a scatter plot matrix for the six most interesting attributes:\n",
    "# Area, eccentricity, equivalent diameter, roundness, compactness, and aspect ratio.\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "bean_attr = [\"Area\", \"Eccentricity\", \"EquivDiameter\", \n",
    "             \"roundness\", \"Compactness\", \"AspectRation\"]\n",
    "scatter_matrix(df[bean_attr], figsize = (12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-compression",
   "metadata": {},
   "source": [
    "The scatter plot matrix above reveal a perfect correlation between the following features:\n",
    "* area vs equivalent diameter\n",
    "* eccentricity vs compactness\n",
    "* eccentricity vs aspect ratio\n",
    "* compactness vs aspect ratio\n",
    "\n",
    "This indicates we do not want to attempt classification solely on the above features. This is because classification is best done when distinct groups can be seen in the data (such as that seen in the area vs eccentricity scatterplot, or area vs roundness).\n",
    "\n",
    "As a result, we should attempt classification on *all* the features provided in the dataset to ensure the best classification methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-trainer",
   "metadata": {},
   "source": [
    "# 2. Viewing class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-treatment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Class\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-plain",
   "metadata": {},
   "source": [
    "We see that there is an imbalance between the classes in the Varieties feature. There is a significantly larger amount of the Dermason variety in our sample in comparison to the other varieties (at about 3500 data points). In comparison, the Bombay variety only has about 600 data points.\n",
    "\n",
    "The issue with class imbalances is non-representative sampling. Ideally, we would have a sample where each bean variety is represented equally.\n",
    "\n",
    "Going forward, it would be useful to implement ML algorithms which would take into account these significant class imbalances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-medicine",
   "metadata": {},
   "source": [
    "# 3. Split to training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size = 0.2, random_state = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set:\", len(train_set.index), \"and testing set:\", len(test_set.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for class imbalances in training set vs test set\n",
    "\n",
    "fig, (subpl1, subpl2) = plt.subplots(1, 2)\n",
    "fig.suptitle(\"Training vs test set\")\n",
    "\n",
    "train_set[\"Class\"].value_counts().plot(kind=\"bar\", ax=subpl1)\n",
    "test_set[\"Class\"].value_counts().plot(kind=\"bar\", ax=subpl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-healing",
   "metadata": {},
   "source": [
    "# 4. Data preparation: feature scaling\n",
    "\n",
    "To prepare our data for our machine learning alogrithms, we'll scale our data using min-max scaling. We'll be using min-max scaling as we don't really have outliers (as per the histograms and scatter plots; refer to EDA in section one). Additionally, SGD is sensitive to feature scaling so we'll stick to using min-max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we only use the training set\n",
    "df = train_set\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric variables only\n",
    "df_num = df.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the min-max scaler as there doesn't seem to be extreme outliers \n",
    "# (as per histogram and scatter plots; see EDA in section 1).\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "x_train = minmax.fit_transform(df_num)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View it in panda's data frame format, just to check\n",
    "pd_num_tr = pd.DataFrame(x_train)\n",
    "pd_num_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the testing set\n",
    "test_num = test_set.select_dtypes(include=[np.number])\n",
    "\n",
    "x_test = minmax.fit_transform(test_num)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-rebel",
   "metadata": {},
   "source": [
    "# 5. Using the Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "y_train = train_set[\"Class\"]\n",
    "y_test = test_set[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-prediction",
   "metadata": {},
   "source": [
    "## 5.1 Choose hyperparameters and optimise\n",
    "\n",
    "We will use GridSearch to find the best hyperparameters for our model. We will play around with the hyperparameters 'kernel', 'gamma', and 'C'.\n",
    "\n",
    "The code in the following section has been commented out due to processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = [\n",
    "#     {'kernel':['linear', 'rbf', 'sigmoid'], \n",
    "#      'gamma':['scale', 'auto'],\n",
    "#      'C':[1,2,3]}\n",
    "# ]\n",
    "\n",
    "# svc = SVC()\n",
    "# grid_search = GridSearchCV(svc, param_grid, cv=5, return_train_score=True)\n",
    "# grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvres = grid_search.cv_results_\n",
    "# pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-conversion",
   "metadata": {},
   "source": [
    "The best hyperparameters found using GridSearch were: <br>\n",
    "* C: 3\n",
    "* Gamma: scale (default)\n",
    "* Kernel: rbf (default)\n",
    "\n",
    "So we will specify these for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-track",
   "metadata": {},
   "source": [
    "## 5.2 Fit the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel=\"rbf\", C=3)\n",
    "svc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc = svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-spare",
   "metadata": {},
   "source": [
    "## 5.3 Cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, plot_confusion_matrix\n",
    "\n",
    "cm_svc = confusion_matrix(y_test, y_pred_svc)\n",
    "cm_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for correct classification\n",
    "# plt.matshow(cm_svc, cmap = plt.cm.gray)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And more specifically:\n",
    "plot_confusion_matrix(svc, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-wichita",
   "metadata": {},
   "source": [
    "The confusion matrix here looks quite good as most values are on the main diagonal, i.e. they were classified correctly (the darker the square, the less values there were classified in that category).\n",
    "\n",
    "The Bombay variety (class '1' in the plot) looks darker than the other varieties; indeed, there were fewer Bombay beans than there were other varieties (refer to the section on EDA in Part 1). We could also check if the classifier does not perform as well on the Bombay than for the other varieties. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on plotting the errors\n",
    "row_sums = cm_svc.sum(axis=1, keepdims=True)\n",
    "ncm_svc = cm_svc / row_sums\n",
    "np.fill_diagonal(ncm_svc, 0)\n",
    "plt.matshow(ncm_svc, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-story",
   "metadata": {},
   "source": [
    "The rows and columns for the Bombay bean (class 1) is dark, indicating that Bombay beans tend to get classified correctly. So we do not need to worry about the classifier not having performed as well on Bombay than for the other varieties.\n",
    "\n",
    "After Bombay beans come the Seker beans (class 5); they also tend to get classified correctly, even though not to the same extent.\n",
    "\n",
    "Demarson and Sira varieties (classes 3 and 6 respectively) tend to get confused for each other, in both directions–moreso Sira beans being misclassified as a Demarson variety than the other way around. This confusion for two varieties also occur for the  Barbunya (class 0) and Cali (class 2) varieties.\n",
    "\n",
    "The column for Sira beans (class 6) is quite bright meaning that other varieties tend to be misclassified as Sira beans. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-climb",
   "metadata": {},
   "source": [
    "# 6. Using the Stochastic Gradient Descent Classifier\n",
    "\n",
    "Comment to self: This is basically a linear regression model.. but slower.\n",
    "\n",
    "Also–it's called 'stochastic' because we randomly sample from the training set and calculate the gradient only for that instance, rather than calculating the gradient at every instance. This method is especially useful for large data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "y_train = train_set[\"Class\"]\n",
    "y_test = test_set[\"Class\"]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-audience",
   "metadata": {},
   "source": [
    "## 6.1 Choose hyperparameters and optimise\n",
    "The hyperparameters we will consider playing around with for this classification method are loss and penalty. I didn't consider trying to change the other hyperparameters as we don't have problems with outliers or any strange data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = [\n",
    "#     {'loss':['hinge', 'log', 'squared_hinge'], # We don't have a lot of outliers so modified_huber not checked\n",
    "#      'penalty':['l2', 'l1', 'elasticnet']}\n",
    "# ]\n",
    "\n",
    "# sgdc = SGDClassifier()\n",
    "# grid_search = GridSearchCV(sgdc, param_grid, cv=5, return_train_score=True)\n",
    "# grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvres = grid_search.cv_results_\n",
    "# pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-thompson",
   "metadata": {},
   "source": [
    "We find that we want the loss parameter to be 'hinge' and penalty hyperparameter to be 'l1'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-start",
   "metadata": {},
   "source": [
    "## 6.2 Train the SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-disclosure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgdc = SGDClassifier(random_state = 35, loss = 'hinge', penalty='l1')\n",
    "sgdc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sgdc.score(x_train, y_train)\n",
    "print(\"Training score: \", score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgdc = sgdc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-dover",
   "metadata": {},
   "source": [
    "## 6.3 Cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_sgdc = confusion_matrix(y_test, y_pred_sgdc)\n",
    "print(cm_sgdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.matshow(cm_sgdc, cmap = plt.cm.gray)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(sgdc, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-military",
   "metadata": {},
   "source": [
    "In this instance the confusion matrix looks a bit worse. Most of the brightness is in the diagonal but there is a high number of errors where Sira beans are misclassified as Seker beans, which is concerning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = cm_sgdc.sum(axis=1, keepdims=True)\n",
    "ncm_sgdc = cm_sgdc / row_sums\n",
    "np.fill_diagonal(ncm_sgdc, 0)\n",
    "plt.matshow(ncm_sgdc, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-attempt",
   "metadata": {},
   "source": [
    "We observe that with this classification method, the most misclassification occured when Seker beans (class 5) are misclassified as Sira (class 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QN-ME: what does cv=3 below indicate for cross_val_score?\n",
    "# It is how many k-folds; in this instance, 3. Default is 5-fold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgdc, x_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-pointer",
   "metadata": {},
   "source": [
    "# 7. Comparing the two classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-picture",
   "metadata": {},
   "source": [
    "We will now compare the two classifiers' confusion matrices side by side. The confusion matrices on the left are that for the SVC whilst the one on the right is for the SGDC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (subpl1, subpl2) = plt.subplots(1, 2)\n",
    "fig.suptitle(\"Confusion Matrices\")\n",
    "\n",
    "subpl1.matshow(cm_svc, cmap = plt.cm.gray)\n",
    "subpl2.matshow(cm_sgdc, cmap = plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-danish",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (subpl1, subpl2) = plt.subplots(1, 2)\n",
    "fig.suptitle(\"Confusion Matrices: Errors\")\n",
    "\n",
    "subpl1.matshow(ncm_svc, cmap=plt.cm.gray)\n",
    "subpl2.matshow(ncm_sgdc, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-hebrew",
   "metadata": {},
   "source": [
    "Ultimately, the SVC seems to be a better classifier for our data as there were less serious misclassifications than in the SGDC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
